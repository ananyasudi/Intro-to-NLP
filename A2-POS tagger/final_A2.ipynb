{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2nA9msSztbx"
      },
      "source": [
        "The task: Language modelling using a neural 4gram model. Basically the input to the neural model is past 4 words and what we try to predict is the 5th word in a sequence.  \n",
        "\n",
        "The dataset used is wikitext, can be downloaded from [this link](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oGNOU1ob7C05"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2bclfP_LFJR",
        "outputId": "c5cbaeeb-b5e0-4eba-cc6a-d9c27439aa47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building vocab: 100%|██████████| 61477/61477 [00:00<00:00, 415403.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'DET': 0, 'NOUN': 1, 'VERB': 2, 'AUX': 3, 'INTJ': 4, 'ADP': 5, 'CCONJ': 6, 'PART': 7, 'NUM': 8, 'PROPN': 9, 'ADV': 10, 'PAD_POS': 11, 'UNK_TAG': 12, 'PRON': 13, 'ADJ': 14}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating data: 100%|██████████| 61477/61477 [00:00<00:00, 840490.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Max length of sentence 46\n",
            "4274\n"
          ]
        }
      ],
      "source": [
        "class Data_set(Dataset):\n",
        "  \n",
        "  def __init__(self, path):\n",
        "    lines=0\n",
        "    \n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "        lines += 1\n",
        "\n",
        "    ud_tags=[]\n",
        "    tokens = []\n",
        "    with open(path, 'r') as f:\n",
        "      for line in tqdm(f, total=lines, desc=\"Building vocab\"):\n",
        "        words=line.split('\\t')\n",
        "        \n",
        "        if line[0].isnumeric()==False:\n",
        "          continue\n",
        "        tokens += [[words[1].lower()]]\n",
        "        ud_tags+=[words[3]]\n",
        "    \n",
        "        \n",
        "    self.vocab = build_vocab_from_iterator(tokens, min_freq=1,\n",
        "                                           specials=['<PAD>','<UNK>'],\n",
        "                                           special_first=False)\n",
        "    \n",
        "    self.vocab.set_default_index(self.vocab.get_stoi()['<UNK>'])\n",
        "    ud_tags+=[\"PAD_POS\",\"UNK_TAG\"]\n",
        "    ud_tags=list(set(ud_tags))\n",
        "    self.pos_tags={tag: idx for idx, tag in enumerate(ud_tags)}\n",
        "    print(self.pos_tags)\n",
        "    max_len=0\n",
        "    with open(path, 'r') as f:\n",
        "      all_sentences=[]\n",
        "      all_tags=[]\n",
        "\n",
        "      sen=[]\n",
        "      tags=[]\n",
        "      for line in tqdm(f, total=lines, desc=\"Generating data\"):\n",
        "        \n",
        "        words=line.split('\\t')\n",
        "        \n",
        "        if line[0].isnumeric()==False:\n",
        "          if len(sen)!=0:\n",
        "            all_sentences.append(sen)\n",
        "            max_len=max(max_len,len(sen))\n",
        "          if len(tags)!=0: all_tags.append(tags)\n",
        "          sen=[]\n",
        "          tags=[]\n",
        "          continue\n",
        "        sen += [words[1].lower()]\n",
        "        tags+= [words[3]]\n",
        "      # all_sentences.append(sen)\n",
        "      # all_tags.append(tags)\n",
        "      print(f\"\\nMax length of sentence {max_len}\")\n",
        "    self.max_len=max_len\n",
        "    for i in range(len(all_sentences)):\n",
        "      all_sentences[i]=[self.vocab[word] for word in all_sentences[i]]\n",
        "      all_tags[i]=[self.pos_tags[tag] for tag in all_tags[i]]\n",
        "      all_sentences[i]+=[self.vocab[\"<PAD>\"]]*(max_len-len(all_sentences[i]))\n",
        "      all_tags[i]+=[self.pos_tags[\"PAD_POS\"]]*(max_len-len(all_tags[i]))\n",
        "\n",
        "    # print(self.vocab.get_stoi())\n",
        "    # print(self.pos_tags.get_stoi())\n",
        "    \n",
        "    self.all_sentences=tensor(all_sentences)\n",
        "    self.all_tags=tensor(all_tags)\n",
        "    print(len(self.all_sentences))\n",
        "    # print(self.all_sentences[-1])\n",
        "\n",
        "    # for i in range(len(all_sentences)):\n",
        "    #   print(f\"{all_sentences[i]}\\n{all_tags[i]}\\n\\n\")\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "      return self.all_sentences.shape[0]\n",
        "      \n",
        "  def __getitem__(self, index : int): # -> tuple[tensor, tensor]\n",
        "      return self.all_sentences[index], self.all_tags[index] # source, target\n",
        "\n",
        "    \n",
        "\n",
        "ds_obj=Data_set('en_atis-ud-train.conllu')\n",
        "torch.save(ds_obj,'ds_obj.pt')\n",
        "ds_obj=torch.load('ds_obj.pt')\n",
        "# ds_obj=Data_set('sample.txt')\n",
        "\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRfLIkGYLos-"
      },
      "source": [
        "## Dataloader\n",
        "\n",
        "- used to load data from the dataset in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEIIDTIULqMD",
        "outputId": "fcfeb345-84f9-45fc-9073-ee867481f941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_dataloader = DataLoader(ds_obj, batch_size=64, shuffle=True)\n",
        "print(len(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zREAEdJsQMEU",
        "outputId": "bae2d251-3a9f-4503-c789-a02763438040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([64, 46])\n",
            "Labels batch shape: torch.Size([64, 46])\n"
          ]
        }
      ],
      "source": [
        "features, labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {features.size()}\")\n",
        "print(f\"Labels batch shape: {labels.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAmScVIENGj9"
      },
      "source": [
        "# The model\n",
        "- init: The architecture\n",
        "- Forward: computing the output given an input. y = Model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuEXU2cMNIKM",
        "outputId": "fafd05ce-bb38-4e0d-e1bd-68ab51852bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# To use GPU \n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YOU9MAg4NnuP"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim,hidden_size, output_dim):\n",
        "    super(LSTMModel, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm=nn.LSTM(embedding_dim,hidden_size,1,batch_first=True)#output would be Batchsize x max_len_sen x hidden_size\n",
        "    self.fc = nn.Linear(hidden_size, output_dim)\n",
        "    self.activ = nn.ReLU()\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "      return self.all_sentences.shape[0]\n",
        "      \n",
        "  def __getitem__(self, index : int): # -> tuple[tensor, tensor]\n",
        "      return self.all_sentences[index], self.all_tags[index] # source, target\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "                    # [bz,len_sen]\n",
        "    # print(f\"Inputs size {inputs.size()}\")\n",
        "    embeds = self.embeddings(inputs)\n",
        "    # [bz, len_sen, 10]\n",
        "    # print(f\"Embeds size {embeds.size()}\")\n",
        "    out,_=self.lstm(embeds)\n",
        "    out = self.fc(out)\n",
        "    # [bz, len_sen, vocab_size]\n",
        "    # print(f\"Output shape {out.shape}\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1b2phx7Ot36"
      },
      "source": [
        "# The training loop \n",
        "- Set hyperparameters\n",
        "- Pick loss function and optimizer\n",
        "- Make train, validation/test loops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXj0okn_Owss",
        "outputId": "e3701942-df00-42fd-bac2-436e617764ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "865\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "vocab_size = len(ds_obj.vocab)\n",
        "print(vocab_size)\n",
        "embedding_dim = 200\n",
        "model =LSTMModel(vocab_size, embedding_dim,150,len(ds_obj.pos_tags))\n",
        "epochs = 6\n",
        "learning_rate = 0.01\n",
        "# batch size is also a hyperparameter\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yIqOczSmkS_",
        "outputId": "50dc3a6f-7272-4b6c-abad-307dbe877ffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Max length of sentence 42\n",
            "Len of all sentences: 573\n",
            "Num of batches 9\n"
          ]
        }
      ],
      "source": [
        "from itertools import chain\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy\n",
        "target_names=[]\n",
        "for i in range(0,len(ds_obj.pos_tags)):\n",
        "  for tag,idx in ds_obj.pos_tags.items():\n",
        "    if idx==i:\n",
        "      target_names.append(tag)\n",
        "\n",
        "def get_whole_data(path,ds_obj):\n",
        "  with open(path, 'r') as f:\n",
        "    all_sentences=[]\n",
        "    all_tags=[]\n",
        "    max_len=0\n",
        "    sen=[]\n",
        "    tags=[]\n",
        "    for line in f.readlines():\n",
        "      \n",
        "      words=line.split('\\t')\n",
        "      \n",
        "      if line[0].isnumeric()==False:\n",
        "        if len(sen)!=0:\n",
        "          all_sentences.append(sen)\n",
        "          max_len=max(max_len,len(sen))\n",
        "        if len(tags)!=0: all_tags.append(tags)\n",
        "        sen=[]\n",
        "        tags=[]\n",
        "        continue\n",
        "      sen += [words[1].lower()]\n",
        "      tags+= [words[3]]\n",
        "    all_sentences.append(sen)\n",
        "    all_tags.append(tags)\n",
        "    print(f\"\\nMax length of sentence {max_len}\")\n",
        "    for i in range(len(all_sentences)):\n",
        "      all_sentences[i]=[ds_obj.vocab[word] for word in all_sentences[i]]\n",
        "      all_tags[i]=[ds_obj.pos_tags.get(tag,ds_obj.pos_tags[\"UNK_TAG\"]) for tag in all_tags[i]]\n",
        "\n",
        "    return all_sentences, all_tags,max_len\n",
        "\n",
        "def get_x_and_y(path,ds_obj,batch_size):\n",
        "  all_sentences,all_tags,max_len=get_whole_data(path,ds_obj)\n",
        "  \n",
        "  # print(all_sentences)\n",
        "  \n",
        "  for i in range(len(all_sentences)):\n",
        "      all_sentences[i]+=[ds_obj.vocab[\"<PAD>\"]]*(max_len-len(all_sentences[i]))\n",
        "      all_tags[i]+=[ds_obj.pos_tags[\"PAD_POS\"]]*(max_len-len(all_tags[i]))\n",
        "  print(f\"Len of all sentences: {len(all_sentences)}\")\n",
        "  comb_data=[]\n",
        "  for i in range(0, len(all_sentences), batch_size):\n",
        "    h=min(i+batch_size,len(all_sentences))\n",
        "    comb_data.append((all_sentences[i:h],all_tags[i:h]))\n",
        "  print(f\"Num of batches {len(comb_data)}\")\n",
        "  \n",
        "  return comb_data\n",
        "  \n",
        "\n",
        "val_data = get_x_and_y('en_atis-ud-dev.conllu',ds_obj,64)\n",
        "# for i,(x_val,y_val) in enumerate(val_data):\n",
        "#   print(len(x_val),len(y_val[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDl6qulegKnU",
        "outputId": "457167b5-23d7-4939-cd94-a30e98f3f40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['DET', 'NOUN', 'VERB', 'AUX', 'INTJ', 'ADP', 'CCONJ', 'PART', 'NUM', 'PROPN', 'ADV', 'PAD_POS', 'UNK_TAG', 'PRON', 'ADJ']\n",
            "{'DET': 0, 'NOUN': 1, 'VERB': 2, 'AUX': 3, 'INTJ': 4, 'ADP': 5, 'CCONJ': 6, 'PART': 7, 'NUM': 8, 'PROPN': 9, 'ADV': 10, 'PAD_POS': 11, 'UNK_TAG': 12, 'PRON': 13, 'ADJ': 14}\n"
          ]
        }
      ],
      "source": [
        "print(target_names)\n",
        "print(ds_obj.pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_VmU98v3YAz",
        "outputId": "0383b95a-2961-4268-a16b-98bd77864cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 67/67 [00:20<00:00,  3.20it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.99      0.87      0.93       568\n",
            "        NOUN       0.98      0.98      0.98      1143\n",
            "        VERB       1.00      0.97      0.98       653\n",
            "         AUX       0.96      0.99      0.97       266\n",
            "        INTJ       1.00      1.00      1.00        35\n",
            "         ADP       0.94      1.00      0.97      1415\n",
            "       CCONJ       1.00      0.99      1.00       107\n",
            "        PART       0.00      0.00      0.00        73\n",
            "         NUM       0.98      0.97      0.98       131\n",
            "       PROPN       0.98      1.00      0.99      1551\n",
            "         ADV       0.96      0.78      0.86        59\n",
            "     PAD_POS       1.00      1.00      1.00     17422\n",
            "     UNK_TAG       0.00      0.00      0.00         2\n",
            "        PRON       0.88      0.99      0.93       414\n",
            "         ADJ       0.95      0.93      0.94       227\n",
            "\n",
            "    accuracy                           0.99     24066\n",
            "   macro avg       0.84      0.83      0.83     24066\n",
            "weighted avg       0.99      0.99      0.99     24066\n",
            "\n",
            "Epoch 1 \t\t Training Loss: 1.8239790183394702 \t\t Validation Loss: 1.8264875411987305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 67/67 [00:21<00:00,  3.12it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.99      0.87      0.93       568\n",
            "        NOUN       0.98      0.98      0.98      1143\n",
            "        VERB       1.00      0.97      0.98       653\n",
            "         AUX       0.96      0.99      0.97       266\n",
            "        INTJ       1.00      1.00      1.00        35\n",
            "         ADP       0.94      1.00      0.97      1415\n",
            "       CCONJ       1.00      0.99      1.00       107\n",
            "        PART       0.00      0.00      0.00        73\n",
            "         NUM       0.98      0.97      0.98       131\n",
            "       PROPN       0.98      1.00      0.99      1551\n",
            "         ADV       0.96      0.78      0.86        59\n",
            "     PAD_POS       1.00      1.00      1.00     17422\n",
            "     UNK_TAG       0.00      0.00      0.00         2\n",
            "        PRON       0.88      0.99      0.93       414\n",
            "         ADJ       0.95      0.93      0.94       227\n",
            "\n",
            "    accuracy                           0.99     24066\n",
            "   macro avg       0.84      0.83      0.83     24066\n",
            "weighted avg       0.99      0.99      0.99     24066\n",
            "\n",
            "Epoch 2 \t\t Training Loss: 1.823982640878478 \t\t Validation Loss: 1.8264875411987305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 67/67 [00:21<00:00,  3.13it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.99      0.87      0.93       568\n",
            "        NOUN       0.98      0.98      0.98      1143\n",
            "        VERB       1.00      0.97      0.98       653\n",
            "         AUX       0.96      0.99      0.97       266\n",
            "        INTJ       1.00      1.00      1.00        35\n",
            "         ADP       0.94      1.00      0.97      1415\n",
            "       CCONJ       1.00      0.99      1.00       107\n",
            "        PART       0.00      0.00      0.00        73\n",
            "         NUM       0.98      0.97      0.98       131\n",
            "       PROPN       0.98      1.00      0.99      1551\n",
            "         ADV       0.96      0.78      0.86        59\n",
            "     PAD_POS       1.00      1.00      1.00     17422\n",
            "     UNK_TAG       0.00      0.00      0.00         2\n",
            "        PRON       0.88      0.99      0.93       414\n",
            "         ADJ       0.95      0.93      0.94       227\n",
            "\n",
            "    accuracy                           0.99     24066\n",
            "   macro avg       0.84      0.83      0.83     24066\n",
            "weighted avg       0.99      0.99      0.99     24066\n",
            "\n",
            "Epoch 3 \t\t Training Loss: 1.8239733994896732 \t\t Validation Loss: 1.8264875411987305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 67/67 [00:21<00:00,  3.10it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.99      0.87      0.93       568\n",
            "        NOUN       0.98      0.98      0.98      1143\n",
            "        VERB       1.00      0.97      0.98       653\n",
            "         AUX       0.96      0.99      0.97       266\n",
            "        INTJ       1.00      1.00      1.00        35\n",
            "         ADP       0.94      1.00      0.97      1415\n",
            "       CCONJ       1.00      0.99      1.00       107\n",
            "        PART       0.00      0.00      0.00        73\n",
            "         NUM       0.98      0.97      0.98       131\n",
            "       PROPN       0.98      1.00      0.99      1551\n",
            "         ADV       0.96      0.78      0.86        59\n",
            "     PAD_POS       1.00      1.00      1.00     17422\n",
            "     UNK_TAG       0.00      0.00      0.00         2\n",
            "        PRON       0.88      0.99      0.93       414\n",
            "         ADJ       0.95      0.93      0.94       227\n",
            "\n",
            "    accuracy                           0.99     24066\n",
            "   macro avg       0.84      0.83      0.83     24066\n",
            "weighted avg       0.99      0.99      0.99     24066\n",
            "\n",
            "Epoch 4 \t\t Training Loss: 1.8239741022907086 \t\t Validation Loss: 1.8264875411987305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 67/67 [00:21<00:00,  3.14it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.99      0.87      0.93       568\n",
            "        NOUN       0.98      0.98      0.98      1143\n",
            "        VERB       1.00      0.97      0.98       653\n",
            "         AUX       0.96      0.99      0.97       266\n",
            "        INTJ       1.00      1.00      1.00        35\n",
            "         ADP       0.94      1.00      0.97      1415\n",
            "       CCONJ       1.00      0.99      1.00       107\n",
            "        PART       0.00      0.00      0.00        73\n",
            "         NUM       0.98      0.97      0.98       131\n",
            "       PROPN       0.98      1.00      0.99      1551\n",
            "         ADV       0.96      0.78      0.86        59\n",
            "     PAD_POS       1.00      1.00      1.00     17422\n",
            "     UNK_TAG       0.00      0.00      0.00         2\n",
            "        PRON       0.88      0.99      0.93       414\n",
            "         ADJ       0.95      0.93      0.94       227\n",
            "\n",
            "    accuracy                           0.99     24066\n",
            "   macro avg       0.84      0.83      0.83     24066\n",
            "weighted avg       0.99      0.99      0.99     24066\n",
            "\n",
            "Epoch 5 \t\t Training Loss: 1.823985791918057 \t\t Validation Loss: 1.8264875411987305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 67/67 [00:20<00:00,  3.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.99      0.87      0.93       568\n",
            "        NOUN       0.98      0.98      0.98      1143\n",
            "        VERB       1.00      0.97      0.98       653\n",
            "         AUX       0.96      0.99      0.97       266\n",
            "        INTJ       1.00      1.00      1.00        35\n",
            "         ADP       0.94      1.00      0.97      1415\n",
            "       CCONJ       1.00      0.99      1.00       107\n",
            "        PART       0.00      0.00      0.00        73\n",
            "         NUM       0.98      0.97      0.98       131\n",
            "       PROPN       0.98      1.00      0.99      1551\n",
            "         ADV       0.96      0.78      0.86        59\n",
            "     PAD_POS       1.00      1.00      1.00     17422\n",
            "     UNK_TAG       0.00      0.00      0.00         2\n",
            "        PRON       0.88      0.99      0.93       414\n",
            "         ADJ       0.95      0.93      0.94       227\n",
            "\n",
            "    accuracy                           0.99     24066\n",
            "   macro avg       0.84      0.83      0.83     24066\n",
            "weighted avg       0.99      0.99      0.99     24066\n",
            "\n",
            "Epoch 6 \t\t Training Loss: 1.8239976363395578 \t\t Validation Loss: 1.8264875411987305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# The main loop \n",
        "\n",
        "print(\"Starting Training...\")\n",
        "e_num=1\n",
        "a=torch.nn.Softmax(dim=1)\n",
        "for epoch in range(epochs):\n",
        "  # Training loop\n",
        "  train_loss = 0\n",
        "  model.train()\n",
        "  num_batch=0\n",
        "  all_pred=[]\n",
        "  all_actual=[]\n",
        "\n",
        "  for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "    x_train, y_train = batch\n",
        "    optimizer.zero_grad()# clears the gradients from prev iteration \n",
        "    # print('train',x_train.shape)\n",
        "    output = model(x_train) # forward pass\n",
        "    output=output.view(-1,output.shape[2])\n",
        "    output = a(output)\n",
        "\n",
        "    # print('test',output.shape,y_train.shape)\n",
        "    loss = loss_fn(output, y_train.view(-1)) # calculate loss\n",
        "\n",
        "    #backprop\n",
        "    loss.backward() # compute gradients\n",
        "    optimizer.step() # update parameters\n",
        "    \n",
        "    train_loss += loss.item()\n",
        "    num_batch+=1\n",
        "\n",
        "    val_loss = 0\n",
        "\n",
        "\n",
        "  # Validation loop \n",
        "  val_loss = 0\n",
        "  model.eval()\n",
        "  for i, (x_val, y_val) in enumerate(val_data):\n",
        "    output = model(tensor(x_val))\n",
        "    output=output.view(-1,output.shape[2])\n",
        "    output=a(output)\n",
        "    all_pred+=[torch.argmax(output[i]) for i in range(len(output))]\n",
        "    all_actual+=list(chain.from_iterable(y_val))\n",
        "    loss = loss_fn(output, tensor(list(chain.from_iterable(y_val))))\n",
        "    val_loss += loss.item()\n",
        "  print(classification_report(all_actual, all_pred, target_names=target_names))\n",
        "  print(f'Epoch {e_num} \\t\\t Training Loss: {train_loss / len(train_dataloader)} \\t\\t Validation Loss: {val_loss / len(val_data)}')\n",
        "  e_num+=1\n",
        "    \n",
        "\n",
        "  # print(train_loss/len(train_dataloader))\n",
        "  # print(f\"Number of batches {num_batch}\")\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wy0WGMJHMjd4"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'trained_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYslD24ZM8oy",
        "outputId": "83b666a4-a2e9-4715-d710-bea9d0a76baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Sentence: what are flights between baltimore and dallas\n",
            "torch.Size([7, 15])\n",
            "what\t PRON\n",
            "are\t AUX\n",
            "flights\t NOUN\n",
            "between\t ADP\n",
            "baltimore\t PROPN\n",
            "and\t CCONJ\n",
            "dallas\t PROPN\n",
            "[tensor(13), tensor(3), tensor(1), tensor(5), tensor(9), tensor(6), tensor(9)]\n"
          ]
        }
      ],
      "source": [
        "model = torch.load('trained_model.pt')\n",
        "x=input(\"Enter Sentence: \")\n",
        "words=x.lower().split(' ')\n",
        "x=tensor([ds_obj.vocab[word] for word in words])#replacing words by indices\n",
        "output=model(x)\n",
        "output=a(output)\n",
        "print(output.shape)\n",
        "output=[torch.argmax(output[i]) for i in range(len(words))]\n",
        "for i in range(len(words)):\n",
        "  for tag,idx in ds_obj.pos_tags.items():\n",
        "    if idx==output[i]:\n",
        "      print(f\"{words[i]}\\t {tag}\")\n",
        "      break\n",
        "  \n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL_NbVu9OmWB",
        "outputId": "b0a1bb4c-5664-4fde-acdd-63f5eade4381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ADP': 0, 'PRON': 1, 'PROPN': 2, 'CCONJ': 3, 'AUX': 4, 'DET': 5, 'ADJ': 6, 'INTJ': 7, 'PART': 8, 'PAD_POS': 9, 'UNK_TAG': 10, 'NOUN': 11, 'NUM': 12, 'VERB': 13, 'ADV': 14}\n"
          ]
        }
      ],
      "source": [
        "print(ds_obj.pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unPYXUrYRAWv",
        "outputId": "864098f6-47d2-45dc-ffb6-64ba4db9670f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Max length of sentence 33\n",
            "Len of all sentences: 587\n",
            "Num of batches 10\n",
            "0.1101912021636963\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.98      0.87      0.92       512\n",
            "        NOUN       1.00      0.99      0.99      1166\n",
            "        VERB       0.99      0.95      0.97       629\n",
            "         AUX       0.92      0.99      0.95       256\n",
            "        INTJ       1.00      1.00      1.00        36\n",
            "         ADP       0.96      1.00      0.98      1434\n",
            "       CCONJ       1.00      0.98      0.99       109\n",
            "        PART       0.00      0.00      0.00        56\n",
            "         NUM       0.94      0.95      0.95       127\n",
            "       PROPN       0.98      1.00      0.99      1567\n",
            "         ADV       1.00      0.66      0.79        76\n",
            "     PAD_POS       1.00      1.00      1.00     12791\n",
            "        PRON       0.86      0.97      0.91       392\n",
            "         ADJ       0.92      0.98      0.95       220\n",
            "\n",
            "    accuracy                           0.99     19371\n",
            "   macro avg       0.90      0.88      0.89     19371\n",
            "weighted avg       0.99      0.99      0.99     19371\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "test_dataset = get_x_and_y('en_atis-ud-test.conllu',ds_obj,64)\n",
        "# target_names.remove('UNK_TAG')\n",
        "val_loss = 0\n",
        "all_pred=[]\n",
        "all_actual=[]\n",
        "model.eval()\n",
        "for i, (x_val, y_val) in enumerate(test_dataset):\n",
        "  output = model(tensor(x_val))\n",
        "  output=output.view(-1,output.shape[2])\n",
        "  all_pred+=[torch.argmax(output[i]) for i in range(len(output))]\n",
        "  all_actual+=list(chain.from_iterable(y_val))\n",
        "  \n",
        "  loss = loss_fn(output, tensor(list(chain.from_iterable(y_val))))\n",
        "  val_loss += loss.item()\n",
        "\n",
        "print(val_loss/len(test_dataset))\n",
        "print(classification_report(all_actual, all_pred, target_names=target_names))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
