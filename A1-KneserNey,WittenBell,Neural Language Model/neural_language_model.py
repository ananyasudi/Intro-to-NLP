# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VAUOJTAsbSHn60gDFEd8FHQWIqjnP76N
"""

import re
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torch.utils.data as data

import math
import numpy as np
torch.set_printoptions(precision=20)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def clean(s):
    """
    return text after replacing mentions, hashtags, urls with 
    appropriate placeholders.
    """
    s=s.lower()
    temp = re.sub("((https?\:\/\/)|(www\.))\S+", "<URL>", s)
    temp=re.sub("\s#\S+","<HASHTAG>",temp)
    temp=re.sub("\s@\S+","<MENTION>",temp)
    temp=re.sub("[a-zA-Z0-9]*@\S+","<EMAIL>",temp)
    temp=re.sub("mr\.","mr",temp)
    temp=re.sub("mrs\.","mrs",temp)
    temp=re.sub("\s+"," ",temp)

    """
    other tokens like can't , aren't ..can also be replaced
    """
    return temp


def tokenise(s):
    """
    return list of tokens 
    """
   
    result=re.findall("<\w+>|\w+|[\.,\"\?\:\;']",s)
    # print(result)
    return result
class nlmodel(nn.Module):
  def __init__(self,inp_rep_dim,hidden_size,batch_size,input_path,num_epochs):
    super().__init__()
    self.inp_dim=inp_rep_dim
    self.hidden_size=hidden_size
    self.batch_size=batch_size
    self.input_path=input_path
    self.vocab=[]
    self.pre_seq=[]
    self.index_of_words={}
    self.pref_seq=[]
    self.num_of_epochs=num_epochs
    self.training_data=[]
    self.testing_data=[]
    self.val_data=[]
    
    
    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # self.to(self.device)
    
  
    # print(embeds)
  

  def initialise_nlms(self):
    self.embedding = nn.Embedding(num_embeddings=len(self.vocab), embedding_dim=self.inp_dim,device=self.device)
    self.lstm = nn.LSTM(input_size=self.inp_dim, hidden_size=self.hidden_size, num_layers=1,batch_first=True)
    self.fc = nn.Linear(self.hidden_size, len(self.vocab),device=self.device)
    
    self.optimiser = torch.optim.SGD(self.parameters(),
                              lr=0.001, momentum=0.01)
    self.to(self.device)


  def generate_vocab(self):
    """
    first clean it and then 
    split using .
    then tokenise it
    """
    f=open(self.input_path,'r')
    max_len_sen=0
    vocab_list=[]
    # sentences=filter(None, cleanedtext.split('.'))#'This is sen1','This is sen2'
    list_of_sentences=[]#[['This','is','sen1'],['This','is','sen2']
    for sentence in f.readlines():
      tokens_list=tokenise(sentence)
      list_of_sentences.append(tokens_list)
      max_len_sen=max(max_len_sen,len(tokens_list))
      for token in tokens_list:
        vocab_list.append(token)
    
    # self.vocab=[w for w in vocab_list if vocab_list.count(w)>=5]
    # self.vocab=list(set(self.vocab))
    # print(len(self.vocab))
    freq={}
    for word in vocab_list:
      freq[word]=freq.get(word,0)+1
    for word,f in freq.items():
      if f>=4:
        self.vocab.append(word)
    # print(len(self.vocab))
    self.index_of_words={word:i for i,word in enumerate(self.vocab)}
    self.index_of_words['unk']=len(self.vocab)
    self.index_of_words['pad']=len(self.vocab)+1
    self.vocab+=['unk','pad']
    
    for sentence in list_of_sentences:
      temp=[['pad']*(max_len_sen-i)+sentence[0:i] for i in range(2,len(sentence)+1)]
      self.pref_seq+=temp
    for i in range(len(self.pref_seq)):
      self.pref_seq[i]=[self.index_of_words.get(word,self.index_of_words['unk'])\
                        for word in self.pref_seq[i]]
   
    # print(len(self.pref_seq))
    self.pref_seq=np.array(self.pref_seq)
    # self.train_data, self.test_data,self.val_data= data.random_split(self.pref_seq, [0.7,0.15,0.15])
    
   
    # self.test_data=self.get_contexts(self.test_data)
    # self.val_data=self.get_contexts(self.val_data)
    # print(f"Length of traindata: {len(self.train_data)}")
    print(f"Length of vocab: {len(self.vocab)}")
    print(f"Length of max sentence:{max_len_sen}")
    print(f"Length of pref seq: {len(self.pref_seq[0])}")
    self.initialise_nlms()

  def get_training_data(self):
    return [self.train_data,self.test_data,self.val_data]
  

  def embeddinglayer(self,contexts):
    return self.embedding(contexts)

  def forward(self,contexts):
    """
    returns output and hiddenlayer
    """
    embeds = self.embeddinglayer(contexts)
    # print(embeds.shape)
    # print(embeds.shape)
    lstm_out, hidden = self.lstm(embeds)
    # print(lstm_out.shape)
    
    outputs = self.fc(lstm_out[:,-1])
    return outputs

  
  


pred=0
actual=0
def train(model):
  """
  divide the pref_sequences into context and result
  Then group them into batch of size=s
  train it for num of times= num of epochs
  """
  criterion= nn.CrossEntropyLoss()
  contexts=np.array(model.pref_seq[:,:-1])
  words=np.array(model.pref_seq[:,-1])
  
 
  for i in range(model.num_of_epochs):
    total_loss=0
    num_batches=(len(model.pref_seq)//model.batch_size)+1
    
    for j in range(num_batches):
      model.optimiser.zero_grad()
      startidx=j*model.batch_size
      endidx=min(startidx+model.batch_size,len(model.pref_seq))
      temp_con=contexts[startidx:endidx]
      temp_wor=words[startidx:endidx]

      predicted=model.forward(torch.tensor(temp_con,device=model.device))
      loss= criterion(predicted,torch.LongTensor(temp_wor).to(model.device))
      total_loss=total_loss+loss.item()
      loss.backward()
      model.optimiser.step()
    
    print(f"Average epoch Loss: {total_loss/num_batches}")




"""
splitting data into train,
test, val and storing them in different files
"""
# train_data_f=open('./train_data_uly_f.txt','w')
# test_data_f=open('./test_data_uly_f.txt','w')
# text_f=open('./Ulysses - James Joyce.txt','r')

# cleanedtext=clean(text_f.read())
# sentences=list(filter(None, cleanedtext.split('.')))
# train_data, test_data= data.random_split(sentences, [0.7,0.3])
# for sen in train_data:
#   train_data_f.write(sen+'\n')
# for sen in test_data:
#   test_data_f.write(sen+'\n')

model=nlmodel(80,90,3000,'train_data_uly_f.txt',3)

model.generate_vocab()

# traindata,testdata,valdata=
train(model)
# find_avg_perplexity(model,'train_data_Pride_f.txt')
# print(model.pref_seq[0])
# print(model.index_of_words['unk'])
# print(get_perp(model,[))

# input = torch.tensor([[1,1,1,1],[1,1,1,1]])
# model.embedding(input)

import math
def find_perplexity(model,contexts):
  """
  returns perplexity of sentence
  given vector of contexts
  """
  # con,words=model.get_contexts(contexts)
  con=contexts[:,:-1]
  words=contexts[:,-1]
  # print(len(con),len(con[0]))
  sum_of_prob=0
  prob_dists=model.forward(torch.tensor(con,device=model.device))
  prob_dists=torch.nn.functional.softmax(prob_dists,dim=0)
  for i in range(len(words)):
    sum_of_prob+=math.log(prob_dists[i][words[i]])
  per=math.exp(sum_of_prob*(-1/len(words)))
  return per

def find_avg_perplexity(model,datafile):
  f=open(datafile,'r')
  res_file=open('./Perplexity_values_test.txt','w')
  list_of_sentences=[tokenise(sen) for sen in f.readlines()]
  
  len1=len(model.pref_seq[0])
  len2=max(len(x) for x in list_of_sentences)
  maxlen=max(int(len1),int(len2))
  total_per=0
  for sentence in list_of_sentences:
    """
    generate prefix sequences and 
    then make it equal to max length
    """
    prefix_seq=[['pad']*(maxlen-i)+sentence[:i] for i in range(2,len(sentence)+1)]
    if len(prefix_seq)<2: continue
    unkidx=model.index_of_words['unk']
    for i in range(len(prefix_seq)):
      prefix_seq[i]=[model.index_of_words.get(word,unkidx) for word in prefix_seq[i]]
    perp=find_perplexity(model,torch.tensor(prefix_seq))
    res_file.write(f"{' '.join(sentence)} {perp} \n")
    total_per+=perp

  print(f"Average perplexity: {total_per/len(list_of_sentences)}")
  return
find_avg_perplexity(model,'./test_data__Pride_f.txt')

